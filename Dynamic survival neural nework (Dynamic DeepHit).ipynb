{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "pd.set_option('display.max_rows', 300, 'display.max_columns', 300)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import adabound\n",
    "from pycox.evaluation.concordance import concordance_td\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter, AalenJohansenFitter\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamicdeephit(nn.Module):\n",
    "    \n",
    "    def __init__(self, fixed_size, timevar_size, ntime, state_size, num_layers=1, atten_size=128, cs_size=128):\n",
    "        super(dynamicdeephit, self).__init__()\n",
    "        \n",
    "        self.fixed_size = fixed_size\n",
    "        self.timevar_size = timevar_size\n",
    "        self.ntime = ntime\n",
    "        self.state_size = state_size\n",
    "        self.atten_size = atten_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cs_size = cs_size\n",
    "        \n",
    "        self.gru = nn.GRU(input_size = self.timevar_size, \n",
    "                          hidden_size = self.state_size,\n",
    "                          num_layers = self.num_layers,\n",
    "                          batch_first = True,\n",
    "                         )\n",
    "        self.linear = nn.Linear(self.state_size, self.timevar_size)\n",
    "        self.attention1 = nn.Linear(self.timevar_size + self.state_size, self.atten_size)\n",
    "        self.attention2 = nn.Linear(self.atten_size, 1)\n",
    "        \n",
    "        self.csnet1_1 = nn.Linear(self.timevar_size + self.state_size + self.fixed_size, self.cs_size)\n",
    "        self.csnet1_2 = nn.Linear(self.cs_size, self.ntime + 2)\n",
    "        \n",
    "        self.csnet2_1 = nn.Linear(self.timevar_size + self.state_size + self.fixed_size, self.cs_size)\n",
    "        self.csnet2_2 = nn.Linear(self.cs_size, self.ntime + 2)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x, input_length):\n",
    "        ## x: (fixed, timevar, obs_mask)\n",
    "        ## input_length: (batch)\n",
    "        ## hc: (batch, state_size)\n",
    "        \n",
    "        fixed, timevar = x\n",
    "        ## fixed: (batch, fixed_size)\n",
    "        ## timevar: (batch, ntime, timevar_size)\n",
    "\n",
    "        batch_size = timevar.shape[0]\n",
    "        \n",
    "        ## SHARED NET\n",
    "        \n",
    "        timevar_multiple = timevar[input_length > 1]\n",
    "        timevar_single = timevar[input_length == 1][:,0,:]\n",
    "        \n",
    "        timevar_multiple_batch_size = timevar_multiple.shape[0]\n",
    "        \n",
    "        timevar_multiple_last = torch.zeros(timevar_multiple_batch_size, self.timevar_size).to(device)\n",
    "        for i in range(timevar_multiple_batch_size):\n",
    "            timevar_multiple_last[i, :] = timevar_multiple[i, (input_length[input_length>1]-1)[i], :]\n",
    "        timevar_last = torch.cat([timevar_multiple_last, timevar_single], axis=0)\n",
    "        \n",
    "        packed_timevar = nn.utils.rnn.pack_padded_sequence(timevar_multiple, input_length[input_length>1]-1, batch_first=True)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, timevar_multiple_batch_size, self.state_size).to(device)\n",
    "        \n",
    "        output, hn = self.gru(packed_timevar, h0)\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=ntime-1)\n",
    "        output_timevar = self.linear(output)\n",
    "        output_timevar = torch.sigmoid(output_timevar)\n",
    "            \n",
    "        decoder_input_with_context_array = []\n",
    "        \n",
    "        for i in range(timevar_multiple_batch_size):\n",
    "        \n",
    "            e_array = []\n",
    "            for t_in in range((input_length[input_length>1]-1)[i]):\n",
    "                decoder_input_with_encoder = torch.cat([timevar_multiple_last[None,i,:], output[None,i,t_in,:]], 1) ## (batch, output_size+state_size)\n",
    "                e = self.attention1(decoder_input_with_encoder) ## (batch, 1)\n",
    "                e = self.attention2(self.activation(e)) ## (batch, 1)\n",
    "                e_array.append(self.activation(e))\n",
    "            e_array = torch.stack(e_array, 1) ## (batch, input_sequence_len, 1)\n",
    "            e_array = torch.softmax(e_array, dim=1) ## (batch, input_sequence_len, 1)\n",
    "            context = torch.sum(e_array * output[None,i,:(input_length[input_length>1]-1)[i],:], dim=1) ## (batch, state_size)\n",
    "\n",
    "            decoder_input_with_context = torch.cat([timevar_multiple_last[None,i,:], context], 1) ## (batch, output_size+state_size)\n",
    "            decoder_input_with_context_array.append(decoder_input_with_context)\n",
    "            \n",
    "        shared_output = torch.cat(decoder_input_with_context_array, 0)\n",
    "        \n",
    "        if timevar_single.shape[0] > 0:\n",
    "            empty_context = torch.zeros(timevar_single.shape[0], self.state_size).to(device)\n",
    "            single_with_empty_context = torch.cat([timevar_single, empty_context], 1)\n",
    "            \n",
    "            shared_output = torch.cat([shared_output, single_with_empty_context], 0)\n",
    "            \n",
    "        shared_output = torch.cat([shared_output, fixed], 1)\n",
    "        \n",
    "        ## CAUSE-SPECIFIC NET\n",
    "        \n",
    "        pred1 = self.csnet1_1(shared_output)\n",
    "        pred1 = self.csnet1_2(self.activation(pred1))\n",
    "        \n",
    "        pred2 = self.csnet2_1(shared_output)\n",
    "        pred2 = self.csnet2_2(self.activation(pred2))\n",
    "        \n",
    "        pred = torch.cat([pred1, pred2], 1)\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        return pred, output_timevar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask3 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask3 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(meas_time)[0], num_Event, num_Category) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
    "    '''\n",
    "        mask4 is required to get the log-likelihood loss \n",
    "        mask4 size is [N, num_Event, num_Category]\n",
    "            if not censored : one element = 1 (0 elsewhere)\n",
    "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(time)[0], num_Event, num_Category) # for the first loss function\n",
    "    for i in range(np.shape(time)[0]):\n",
    "        if label[i,0] != 0:  #not censored\n",
    "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "        else: #label[i,2]==0: censored\n",
    "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask3(time, meas_time, num_Category):\n",
    "    '''\n",
    "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
    "        mask5 size is [N, num_Category]. \n",
    "        - For longitudinal measurements:\n",
    "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
    "             denom is not needed since comparing is done over the same denom\n",
    "        - For single measurement:\n",
    "             1's from start to the event time(inclusive)\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(time)[0], num_Category) # for the first loss function\n",
    "    if np.shape(meas_time):  #lonogitudinal measurements \n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t1 = int(meas_time[i, 0]) # last measurement time\n",
    "            t2 = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    else:                    #single measurement\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pmf, timevar, input_length, obs_mask, long_pred, event, time, mask1, mask2, mask3, eps=1.e-9, alpha=0.1, beta=0.1, num_event=2):\n",
    "  \n",
    "    ## loss 1\n",
    "    I_1 = torch.sign(event)\n",
    "    \n",
    "    denom = 1 - torch.sum(torch.sum(mask1 * pmf, dim=2), dim=1, keepdims=True)\n",
    "    denom.clamp_(eps, 1-eps)\n",
    "\n",
    "    logpdf = I_1 * torch.log(torch.sum(torch.sum(mask2 * pmf, dim=2), dim=1, keepdims=True) / denom + eps)\n",
    "    logsurv = (1. - I_1) * torch.log(torch.sum(torch.sum(mask2 * pmf, dim=2), dim=1, keepdims=True) / denom + eps)\n",
    "\n",
    "    loss_1 = - torch.mean(logpdf + 1.0*logsurv)\n",
    "\n",
    "    ## loss 2\n",
    "    sigma1 = 0.1\n",
    "\n",
    "    eta = []\n",
    "    mask3s = torch.stack(num_event * [mask3], 1)\n",
    "    for e in range(num_event):\n",
    "        one_vector = torch.ones_like(event)\n",
    "        I_2 = (event == e+1).float()\n",
    "        I_2 = torch.diag(torch.squeeze(I_2))\n",
    "        \n",
    "        R = torch.matmul(pmf[:,e,:], torch.t(mask3s[:,e,:]))\n",
    "        ## R_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. R_i(T_{j})\n",
    "\n",
    "        diag_R = torch.reshape(torch.diag(R), [-1, 1])\n",
    "        R = torch.matmul(one_vector, torch.t(diag_R)) - R\n",
    "        R = torch.t(R)\n",
    "\n",
    "        T = torch.relu(torch.sign(torch.matmul(one_vector, torch.t(time)) - torch.matmul(time, torch.t(one_vector))))\n",
    "        ## T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "        T = torch.matmul(I_2, T)\n",
    "\n",
    "        tmp_eta = torch.mean(T * torch.exp(-R/sigma1), dim=1, keepdims=True)\n",
    "\n",
    "        eta.append(tmp_eta)\n",
    "\n",
    "    eta = torch.stack(eta, dim=1)\n",
    "    eta = torch.mean(torch.reshape(eta, [-1, num_event]), dim=1, keepdims=True)\n",
    "\n",
    "    loss_2 = torch.sum(eta)\n",
    "    \n",
    "    ## loss 3\n",
    "\n",
    "    timevar_multiple = timevar[input_length > 1,1:,:]\n",
    "    \n",
    "    # Cross entropy loss (for categorical)\n",
    "    loss_3 = -torch.sum((timevar_multiple[:,:,:19]*torch.log(long_pred[:,:,:19] + eps) + (1 - timevar_multiple[:,:,:19])*torch.log(1 - long_pred[:,:,:19] + eps))*obs_mask[input_length > 1,1:,:19])/obs_mask[input_length > 1,1:,:19].sum()\n",
    "    # Mean square loss (for continuous)\n",
    "    loss_3 += torch.sum((timevar_multiple[:,:,19:] - long_pred[:,:,19:])**2*obs_mask[input_length > 1,1:,19:])/obs_mask[input_length > 1,1:,19:].sum()\n",
    "    \n",
    "    return loss_1, alpha * loss_2, beta * loss_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for weight_decay in [1e-3]:\n",
    "    for state_size in [128, 256]:\n",
    "        for num_layers in [1, 2]:\n",
    "            for atten_size in [128, 256]:\n",
    "                for cs_size in [128, 256]:\n",
    "\n",
    "                    path = './DynamicDeepHit_{}statesize_{}numlayers_{}attensize_{}cssize_{:.0e}'.format(state_size, num_layers, atten_size, cs_size, weight_decay)\n",
    "                    #if os.path.isfile(path):\n",
    "                    #    continue\n",
    "\n",
    "                    model = dynamicdeephit(fixed_train.shape[-1], timevar_compact_train.shape[-1], ntime, state_size, num_layers, atten_size, cs_size).to(device)\n",
    "                    #if os.path.isfile(path):\n",
    "                    #    model.load_state_dict(torch.load(path, map_location = device))\n",
    "\n",
    "                    lr = 1e-2\n",
    "                    optimizer = adabound.AdaBound(model.parameters(), lr=lr, final_lr=0.1, weight_decay=0)\n",
    "\n",
    "                    loss_array = []\n",
    "                    loss_array1 = []\n",
    "                    loss_array2 = []\n",
    "                    loss_array3 = []\n",
    "                    patience = 0\n",
    "                    min_loss = np.inf\n",
    "                    for e in range(int(1e6)):\n",
    "\n",
    "                        loss1_array_tmp = []\n",
    "                        loss2_array_tmp = []\n",
    "                        loss3_array_tmp = []\n",
    "\n",
    "                        for fixed_batch, timevar_compact_batch, event_batch, time_batch, obs_mask_batch in train_loader:\n",
    "\n",
    "                            input_lengths_batch = ((timevar_compact_batch[:,:,-1] * train_max_timevar[0,0,-1]).sum(1) + 1).int()\n",
    "                            input_lengths_batch, sorted_idx = input_lengths_batch.sort(0, descending=True)\n",
    "\n",
    "                            fixed_batch_sort = fixed_batch[sorted_idx].float()\n",
    "                            timevar_compact_batch_sort = timevar_compact_batch[sorted_idx].float()\n",
    "                            event_batch_sort = event_batch[sorted_idx].float()\n",
    "                            time_batch_sort = time_batch[sorted_idx].float()\n",
    "                            obs_mask_batch_sort = obs_mask_batch[sorted_idx].float()\n",
    "\n",
    "                            mask1_batch_sort = f_get_fc_mask1(input_lengths_batch.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2)\n",
    "                            mask2_batch_sort = f_get_fc_mask2(time_batch_sort.reshape(-1, 1), event_batch_sort.reshape(-1, 1), num_Event=2, num_Category=ntime+2)\n",
    "                            mask3_batch_sort = f_get_fc_mask3(time_batch_sort.reshape(-1, 1), input_lengths_batch.reshape(-1, 1) - 1, ntime+2)\n",
    "\n",
    "                            y_pred, long_pred = model((fixed_batch_sort.to(device), timevar_compact_batch_sort.to(device)), input_lengths_batch)\n",
    "\n",
    "                            norm = 0.\n",
    "                            for parameter in model.parameters():\n",
    "                                norm += torch.norm(parameter, p=1)\n",
    "\n",
    "                            loss1, loss2, loss3 = custom_loss(pmf = y_pred.reshape(-1, 2, ntime+2), \n",
    "                                                              timevar = timevar_compact_batch_sort.to(device),\n",
    "                                                              input_length = input_lengths_batch.to(device),\n",
    "                                                              obs_mask = obs_mask_batch_sort.to(device),\n",
    "                                                              long_pred = long_pred.to(device), \n",
    "                                                              event = event_batch_sort.reshape(-1, 1).to(device), \n",
    "                                                              time = time_batch_sort.reshape(-1, 1).to(device),\n",
    "                                                              mask1 = mask1_batch_sort.to(device), \n",
    "                                                              mask2 = mask2_batch_sort.to(device), \n",
    "                                                              mask3 = mask3_batch_sort.to(device))\n",
    "\n",
    "                            loss = loss1 + loss2 + loss3 + weight_decay*norm\n",
    "                            loss1_array_tmp.append(loss1.item())\n",
    "                            loss2_array_tmp.append(loss2.item())\n",
    "                            loss3_array_tmp.append(loss3.item())\n",
    "\n",
    "                            model.zero_grad()\n",
    "\n",
    "                            loss.backward()\n",
    "\n",
    "                            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                            optimizer.step()\n",
    "\n",
    "                        loss_array.append(np.mean(loss1_array_tmp) + np.mean(loss2_array_tmp) + np.mean(loss3_array_tmp))\n",
    "                        loss_array1.append(np.mean(loss1_array_tmp))\n",
    "                        loss_array2.append(np.mean(loss2_array_tmp))\n",
    "                        loss_array3.append(np.mean(loss3_array_tmp))\n",
    "                        if e % 100 == 0:\n",
    "                            print('Epoch: ' + str(e) + \n",
    "                                  ', TotalLoss: '+ f'{loss_array[-1]:.4e}' +\n",
    "                                  ', Loss1: '+ f'{loss_array1[-1]:.4e}' + \n",
    "                                  ', Loss2: '+ f'{loss_array2[-1]:.4e}' + \n",
    "                                  ', Loss3: '+ f'{loss_array3[-1]:.4e}')\n",
    "                        if min_loss > loss_array[-1]:\n",
    "                            patience = 0\n",
    "                            min_loss = loss_array[-1]\n",
    "                            torch.save(model.state_dict(), path)\n",
    "                        else:\n",
    "                            patience += 1\n",
    "\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                        if patience > 1000:\n",
    "                            break\n",
    "\n",
    "                    plt.plot(loss_array, label='Total Loss')\n",
    "                    plt.plot(loss_array1, label='Loss 1')\n",
    "                    plt.plot(loss_array2, label='Loss 2')\n",
    "                    plt.plot(loss_array3, label='Loss 3')\n",
    "                    plt.ylabel('loss')\n",
    "                    plt.xlabel('epoch')\n",
    "                    #plt.yscale('log')\n",
    "                    plt.title(path[2:])\n",
    "                    plt.legend()\n",
    "                    plt.show()\n",
    "\n",
    "                    model.load_state_dict(torch.load(path, map_location = device))\n",
    "                    \n",
    "                    print(path[9:])\n",
    "                    \n",
    "                    for delt in [1, 3, 6, 9, 12]:\n",
    "\n",
    "                        print('****************************************')\n",
    "                        print('Delta t = {}'.format(delt))\n",
    "                        print('****************************************')\n",
    "\n",
    "                        input_lengths_train = torch.IntTensor((timevar_compact_train[:,:,-1] * train_max_timevar[0,0,-1]).sum(1) + 1)\n",
    "                        input_lengths_train, sorted_idx = input_lengths_train.sort(0, descending=True)\n",
    "\n",
    "                        fixed_train_sort = torch.tensor(fixed_train)[sorted_idx].float()\n",
    "                        timevar_compact_train_sort = torch.tensor(timevar_compact_train)[sorted_idx].float()\n",
    "                        event_train_sort = torch.tensor(event_train)[sorted_idx].float()\n",
    "                        time_train_sort = torch.tensor(time_train)[sorted_idx].float()\n",
    "                        obs_mask_compact_train_sort = torch.tensor(obs_mask_compact_train)[sorted_idx].float()\n",
    "\n",
    "                        input_lengths_test = torch.IntTensor((timevar_compact_test[:,:,-1] * train_max_timevar[0,0,-1]).sum(1) + 1)\n",
    "                        input_lengths_test, sorted_idx = input_lengths_test.sort(0, descending=True)\n",
    "\n",
    "                        fixed_test_sort = torch.tensor(fixed_test)[sorted_idx].float()\n",
    "                        timevar_compact_test_sort = torch.tensor(timevar_compact_test)[sorted_idx].float()\n",
    "                        event_test_sort = torch.tensor(event_test)[sorted_idx].float()\n",
    "                        time_test_sort = torch.tensor(time_test)[sorted_idx].float()\n",
    "                        obs_mask_compact_test_sort = torch.tensor(obs_mask_compact_test)[sorted_idx].float()\n",
    "\n",
    "                        y_train, _ = model((fixed_train_sort.to(device), timevar_compact_train_sort.to(device)), input_lengths_train)\n",
    "                        y_test, _ = model((fixed_test_sort.to(device), timevar_compact_test_sort.to(device)), input_lengths_test)\n",
    "\n",
    "                        mask1_train_sort = f_get_fc_mask1(input_lengths_train.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2).to(device)\n",
    "                        mask1_test_sort = f_get_fc_mask1(input_lengths_test.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2).to(device)\n",
    "\n",
    "                        CIF_train = torch.cumsum(y_train.reshape(-1,2,ntime+2) * (1-mask1_train_sort), 2)\n",
    "                        CIF_test = torch.cumsum(y_test.reshape(-1,2,ntime+2) * (1-mask1_test_sort), 2)\n",
    "\n",
    "                        time_train_trunc = np.int32(np.where(time_train_sort.detach().cpu().numpy() > delt-1, delt-1, time_train_sort.detach().cpu().numpy()))\n",
    "                        time_test_trunc = np.int32(np.where(time_test_sort.detach().cpu().numpy() > delt-1, delt-1, time_test_sort.detach().cpu().numpy()))\n",
    "\n",
    "                        event_train_trunc = np.int32(np.where(time_train_sort.detach().cpu().numpy() > delt-1, 0, event_train_sort.detach().cpu().numpy()))\n",
    "                        event_test_trunc = np.int32(np.where(time_test_sort.detach().cpu().numpy() > delt-1, 0, event_test_sort.detach().cpu().numpy()))\n",
    "\n",
    "                        print('train ctd for CVD: {:.4f}'.format(concordance_td(time_train_trunc, event_train_trunc==1, 1.-CIF_train[:,0,:].detach().cpu().numpy().T, time_train_trunc)))\n",
    "                        print('train ctd for Death: {:.4f}'.format(concordance_td(time_train_trunc, event_train_trunc==2, 1.-CIF_train[:,1,:].detach().cpu().numpy().T, time_train_trunc)))\n",
    "\n",
    "                        print('test ctd for CVD: {:.4f}'.format(concordance_td(time_test_trunc, event_test_trunc==1, 1.-CIF_test[:,0,:].detach().cpu().numpy().T, time_test_trunc)))\n",
    "                        print('test ctd for Death: {:.4f}'.format(concordance_td(time_test_trunc, event_test_trunc==2, 1.-CIF_test[:,1,:].detach().cpu().numpy().T, time_test_trunc)) + '\\n')\n",
    "\n",
    "                        E_CVD_train = np.array([CIF_train[i,0,j].item() for (i,j) in zip(range(len(time_train_trunc)), time_train_trunc)])\n",
    "                        O_CVD_train = event_train_trunc==1\n",
    "                        E_Death_train = np.array([CIF_train[i,1,j].item() for (i,j) in zip(range(len(time_train_trunc)), time_train_trunc)])\n",
    "                        O_Death_train = event_train_trunc==2\n",
    "\n",
    "                        E_CVD_test = np.array([CIF_test[i,0,j].item() for (i,j) in zip(range(len(time_test_trunc)), time_test_trunc)])\n",
    "                        O_CVD_test = event_test_trunc==1\n",
    "                        E_Death_test = np.array([CIF_test[i,1,j].item() for (i,j) in zip(range(len(time_test_trunc)), time_test_trunc)])\n",
    "                        O_Death_test = event_test_trunc==2\n",
    "\n",
    "                        print('train Brier for CVD: {:.4f}'.format(((E_CVD_train - O_CVD_train)**2).mean()))\n",
    "                        print('train Brier for Death: {:.4f}'.format(((E_Death_train - O_Death_train)**2).mean()))\n",
    "\n",
    "                        print('test Brier for CVD: {:.4f}'.format(((E_CVD_test - O_CVD_test)**2).mean()))\n",
    "                        print('test Brier for Death: {:.4f}'.format(((E_Death_test - O_Death_test)**2).mean()) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
