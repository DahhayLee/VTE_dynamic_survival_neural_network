{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "import datetime\n",
    "\n",
    "pd.set_option('display.max_rows', 300, 'display.max_columns', 300)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import adabound\n",
    "from pycox.evaluation.concordance import concordance_td\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from lifelines import KaplanMeierFitter, NelsonAalenFitter, AalenJohansenFitter\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cause-specific prediction (single event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_get_fc_mask1(meas_time, num_Event, num_Category):\n",
    "    '''\n",
    "        mask3 is required to get the contional probability (to calculate the denominator part)\n",
    "        mask3 size is [N, num_Event, num_Category]. 1's until the last measurement time\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(meas_time)[0], num_Event, num_Category) # for denominator\n",
    "    for i in range(np.shape(meas_time)[0]):\n",
    "        mask[i, :, :int(meas_time[i, 0]+1)] = 1 # last measurement time\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask2(time, label, num_Event, num_Category):\n",
    "    '''\n",
    "        mask4 is required to get the log-likelihood loss \n",
    "        mask4 size is [N, num_Event, num_Category]\n",
    "            if not censored : one element = 1 (0 elsewhere)\n",
    "            if censored     : fill elements with 1 after the censoring time (for all events)\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(time)[0], num_Event, num_Category) # for the first loss function\n",
    "    for i in range(np.shape(time)[0]):\n",
    "        if label[i,0] != 0:  #not censored\n",
    "            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n",
    "        else: #label[i,2]==0: censored\n",
    "            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def f_get_fc_mask3(time, meas_time, num_Category):\n",
    "    '''\n",
    "        mask5 is required calculate the ranking loss (for pair-wise comparision)\n",
    "        mask5 size is [N, num_Category]. \n",
    "        - For longitudinal measurements:\n",
    "             1's from the last measurement to the event time (exclusive and inclusive, respectively)\n",
    "             denom is not needed since comparing is done over the same denom\n",
    "        - For single measurement:\n",
    "             1's from start to the event time(inclusive)\n",
    "    '''\n",
    "    mask = torch.zeros(np.shape(time)[0], num_Category) # for the first loss function\n",
    "    if np.shape(meas_time):  #lonogitudinal measurements \n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t1 = int(meas_time[i, 0]) # last measurement time\n",
    "            t2 = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    else:                    #single measurement\n",
    "        for i in range(np.shape(time)[0]):\n",
    "            t = int(time[i, 0]) # censoring/event time\n",
    "            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(pmf, event, time, mask1, mask2, mask3, eps=1.e-9, alpha=0.1, num_event=1):\n",
    "       \n",
    "    ## loss 1\n",
    "    I_1 = torch.sign(event)\n",
    "    \n",
    "    denom = 1 - torch.sum(torch.sum(mask1 * pmf, dim=2), dim=1, keepdims=True)\n",
    "    denom.clamp_(eps, 1-eps)\n",
    "\n",
    "    logpdf = I_1 * torch.log(torch.sum(torch.sum(mask2 * pmf, dim=2), dim=1, keepdims=True) / denom + eps)\n",
    "    logsurv = (1. - I_1) * torch.log(torch.sum(torch.sum(mask2 * pmf, dim=2), dim=1, keepdims=True) / denom + eps)\n",
    "\n",
    "    loss_1 = - torch.mean(logpdf + 1.0*logsurv)\n",
    "\n",
    "    ## loss 2\n",
    "    sigma1 = 0.1\n",
    "\n",
    "    eta = []\n",
    "    mask3s = torch.stack(num_event * [mask3], 1)\n",
    "    for e in range(num_event):\n",
    "        one_vector = torch.ones_like(event)\n",
    "        I_2 = (event == e+1).float()\n",
    "        I_2 = torch.diag(torch.squeeze(I_2))\n",
    "        \n",
    "        R = torch.matmul(pmf[:,e,:], torch.t(mask3s[:,e,:]))\n",
    "        ## R_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. R_i(T_{j})\n",
    "\n",
    "        diag_R = torch.reshape(torch.diag(R), [-1, 1])\n",
    "        R = torch.matmul(one_vector, torch.t(diag_R)) - R\n",
    "        R = torch.t(R)\n",
    "\n",
    "        T = torch.relu(torch.sign(torch.matmul(one_vector, torch.t(time)) - torch.matmul(time, torch.t(one_vector))))\n",
    "        ## T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n",
    "\n",
    "        T = torch.matmul(I_2, T)\n",
    "\n",
    "        tmp_eta = torch.mean(T * torch.exp(-R/sigma1), dim=1, keepdims=True)\n",
    "\n",
    "        eta.append(tmp_eta)\n",
    "\n",
    "    eta = torch.stack(eta, dim=1)\n",
    "    eta = torch.mean(torch.reshape(eta, [-1, num_event]), dim=1, keepdims=True)\n",
    "\n",
    "    loss_2 = torch.sum(eta)\n",
    "\n",
    "    return loss_1, alpha * loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deephitCS(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, ntime, shared_size=128):\n",
    "        super(deephitCS, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.ntime = ntime\n",
    "        \n",
    "        self.shared_size = shared_size\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.shared_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(self.shared_size, self.ntime+2)\n",
    "        )\n",
    "        #self.cs1 = nn.Sequential(\n",
    "        #    nn.Linear(self.shared_size + self.input_size, self.cs_size),\n",
    "        #    Mish(),\n",
    "        #    nn.Linear(self.cs_size, self.ntime+1)\n",
    "        #)\n",
    "                \n",
    "    def forward(self, x0):\n",
    "        ## x: (batch, input_size)\n",
    "\n",
    "        batch_size = x0.shape[0]\n",
    "        \n",
    "        ## SHARED NET\n",
    "        \n",
    "        x = self.shared(x0)\n",
    "        \n",
    "        return torch.softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_decay in [1e-3]:\n",
    "    for shared_size in [16, 32, 64, 128, 256]:\n",
    "\n",
    "        path = './DeepHit_cs_{}statesize_{:.0e}'.format(shared_size, weight_decay)\n",
    "        #if os.path.isfile(path):\n",
    "        #    continue\n",
    "\n",
    "        model = deephitCS(input_size=total_train.shape[-1], ntime=ntime, shared_size=shared_size).to(device)\n",
    "        #if os.path.isfile(path):\n",
    "        #    model.load_state_dict(torch.load(path, map_location = device))\n",
    "\n",
    "        lr = 1e-3\n",
    "        optimizer = adabound.AdaBound(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "        loss_array = []\n",
    "        loss_array1 = []\n",
    "        loss_array2 = []\n",
    "        patience = 0\n",
    "        min_loss = np.inf\n",
    "        for e in range(int(1e6)):\n",
    "\n",
    "            loss1_array_tmp = []\n",
    "            loss2_array_tmp = []\n",
    "\n",
    "            for total_batch, timevar_compact_batch, event_batch, time_batch, obs_mask_batch in train_loader:\n",
    "\n",
    "                \n",
    "                input_lengths_batch = (timevar_compact_batch[:,:,-1].sum(1) + 1).int()\n",
    "\n",
    "                total_batch = total_batch.float()\n",
    "                event_batch = event_batch.float()\n",
    "                time_batch = time_batch.float()\n",
    "                obs_mask_batch = obs_mask_batch.float()        \n",
    "                \n",
    "                mask1_batch = f_get_fc_mask1(input_lengths_batch.reshape(-1, 1) - 1, num_Event=1, num_Category=ntime+2)\n",
    "                mask2_batch = f_get_fc_mask2(time_batch.reshape(-1, 1), event_batch.reshape(-1, 1), num_Event=1, num_Category=ntime+2)\n",
    "                mask3_batch = f_get_fc_mask3(time_batch.reshape(-1, 1), input_lengths_batch.reshape(-1, 1) - 1, ntime+2)\n",
    "\n",
    "                y_pred = model(total_batch.to(device))\n",
    "\n",
    "                norm = 0.\n",
    "                for parameter in model.parameters():\n",
    "                    norm += torch.norm(parameter, p=1)\n",
    "\n",
    "                loss1, loss2 = custom_loss(pmf = y_pred.reshape(-1, num_event, ntime+2), \n",
    "                                           event = event_batch.reshape(-1, 1).to(device), \n",
    "                                           time = time_batch.reshape(-1, 1).to(device),\n",
    "                                           mask1 = mask1_batch.to(device), \n",
    "                                           mask2 = mask2_batch.to(device),  \n",
    "                                           mask3 = mask3_batch.to(device), \n",
    "                                           num_event = num_event)\n",
    "\n",
    "                loss = loss1 + loss2 + weight_decay*norm\n",
    "                loss1_array_tmp.append(loss1.item())\n",
    "                loss2_array_tmp.append(loss2.item())\n",
    "\n",
    "                model.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                optimizer.step()\n",
    "\n",
    "            loss_array.append(np.mean(loss1_array_tmp) + np.mean(loss2_array_tmp))\n",
    "            loss_array1.append(np.mean(loss1_array_tmp))\n",
    "            loss_array2.append(np.mean(loss2_array_tmp))\n",
    "            if e % 100 == 0:\n",
    "                print('Epoch: ' + str(e) + \n",
    "                      ', TotalLoss: '+ f'{loss_array[-1]:.4e}' +\n",
    "                      ', Loss1: '+ f'{loss_array1[-1]:.4e}' + \n",
    "                      ', Loss2: '+ f'{loss_array2[-1]:.4e}')\n",
    "            if min_loss > loss_array[-1]:\n",
    "                patience = 0\n",
    "                min_loss = loss_array[-1]\n",
    "                torch.save(model.state_dict(), path)\n",
    "            else:\n",
    "                patience += 1\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if patience > 1000:\n",
    "                break\n",
    "\n",
    "        plt.plot(loss_array, label='Total Loss')\n",
    "        plt.plot(loss_array1, label='Loss 1')\n",
    "        plt.plot(loss_array2, label='Loss 2')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.yscale('log')\n",
    "        plt.title(path[2:])\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        model.load_state_dict(torch.load(path, map_location = device))\n",
    "        \n",
    "        print(path[9:])\n",
    "        \n",
    "        for delt in [1,3,6,9,12]:\n",
    "\n",
    "            print('****************************************')\n",
    "            print('Delta t = {}'.format(delt))\n",
    "            print('****************************************')\n",
    "\n",
    "            input_lengths_train = torch.IntTensor((timevar_compact_train[:,:,-1]).sum(1) + 1)\n",
    "\n",
    "            total_train_sort = torch.FloatTensor(total_train)\n",
    "            event_train_sort = torch.FloatTensor(event_train)\n",
    "            time_train_sort = torch.FloatTensor(time_train)\n",
    "            obs_mask_compact_train_sort = torch.FloatTensor(obs_mask_compact_train)\n",
    "\n",
    "            input_lengths_test = torch.IntTensor((timevar_compact_test[:,:,-1] ).sum(1) + 1)\n",
    "\n",
    "            total_test_sort = torch.FloatTensor(total_test)\n",
    "            event_test_sort = torch.FloatTensor(event_test)\n",
    "            time_test_sort = torch.FloatTensor(time_test)\n",
    "            obs_mask_compact_test_sort = torch.FloatTensor(obs_mask_compact_test)\n",
    "\n",
    "            y_train = model(total_train_sort.to(device))\n",
    "            y_test = model(total_test_sort.to(device))\n",
    "\n",
    "            mask1_train_sort = f_get_fc_mask1(input_lengths_train.reshape(-1, 1) - 1, num_Event=1, num_Category=ntime+2).to(device)\n",
    "            mask1_test_sort = f_get_fc_mask1(input_lengths_test.reshape(-1, 1) - 1, num_Event=1, num_Category=ntime+2).to(device)\n",
    "\n",
    "            CIF_train = torch.cumsum(y_train.reshape(-1,1,ntime+2) * (1-mask1_train_sort), 2)\n",
    "            CIF_test = torch.cumsum(y_test.reshape(-1,1,ntime+2) * (1-mask1_test_sort), 2)\n",
    "\n",
    "            print('train ctd for CVD: {:.4f}'.format(concordance_td(time_train_sort.detach().cpu().numpy(), event_train_sort.detach().cpu().numpy()==1, 1.-CIF_train[:,0,:].detach().cpu().numpy().T, np.int32(time_train_sort.detach().cpu().numpy()-1))))\n",
    "            #print('train ctd for Death: {:.4f}'.format(concordance_td(time_train_sort.detach().cpu().numpy(), event_train_sort.detach().cpu().numpy()==2, 1.-CIF_train[:,1,:].detach().cpu().numpy().T, np.int32(time_train_sort.detach().cpu().numpy()-1))))\n",
    "\n",
    "            print('test ctd for CVD: {:.4f}'.format(concordance_td(time_test_sort.detach().cpu().numpy(), event_test_sort.detach().cpu().numpy()==1, 1.-CIF_test[:,0,:].detach().cpu().numpy().T, np.int32(time_test_sort.detach().cpu().numpy()-1))) + '\\n')\n",
    "            #print('test ctd for Death: {:.4f}'.format(concordance_td(time_test_sort.detach().cpu().numpy(), event_test_sort.detach().cpu().numpy()==2, 1.-CIF_test[:,1,:].detach().cpu().numpy().T, np.int32(time_test_sort.detach().cpu().numpy()-1))) + '\\n')\n",
    "\n",
    "            E_CVD_train = np.array([CIF_train[i,0,int(j)].item() for (i,j) in zip(range(len(time_train_sort)), time_train_sort.detach().cpu().numpy()-1)])\n",
    "            O_CVD_train = event_train_sort.detach().cpu().numpy()==1\n",
    "            #E_Death_train = np.array([CIF_train[i,1,int(j)].item() for (i,j) in zip(range(len(time_train_sort)), time_train_sort.detach().cpu().numpy()-1)])\n",
    "            #O_Death_train = event_train_sort.detach().cpu().numpy()==2\n",
    "\n",
    "            E_CVD_test = np.array([CIF_test[i,0,int(j)].item() for (i,j) in zip(range(len(time_test_sort)), time_test_sort.detach().cpu().numpy()-1)])\n",
    "            O_CVD_test = event_test_sort.detach().cpu().numpy()==1\n",
    "            #E_Death_test = np.array([CIF_test[i,1,int(j)].item() for (i,j) in zip(range(len(time_test_sort)), time_test_sort.detach().cpu().numpy()-1)])\n",
    "            #O_Death_test = event_test_sort.detach().cpu().numpy()==2\n",
    "\n",
    "            print('train Brier for CVD: {:.4f}'.format(((E_CVD_train - O_CVD_train)**2).mean()))\n",
    "            #print('train Brier for Death: {:.4f}'.format(((E_Death_train - O_Death_train)**2).mean()))\n",
    "\n",
    "            print('test Brier for CVD: {:.4f}'.format(((E_CVD_test - O_CVD_test)**2).mean()) + '\\n')\n",
    "            #print('test Brier for Death: {:.4f}'.format(((E_Death_test - O_Death_test)**2).mean()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competing risks models (multiple events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deephitCR(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, ntime, shared_size=128, cs_size=128, num_event=2):\n",
    "        super(deephitCR, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.ntime = ntime\n",
    "        \n",
    "        self.shared_size = shared_size\n",
    "        self.cs_size = cs_size\n",
    "        self.num_event = num_event\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.shared_size),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "        self.cs1 = nn.Sequential(\n",
    "            nn.Linear(self.shared_size + self.input_size, self.cs_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(self.cs_size, self.ntime+2)\n",
    "        )\n",
    "        self.cs2 = nn.Sequential(\n",
    "            nn.Linear(self.shared_size + self.input_size, self.cs_size),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(self.cs_size, self.ntime+2)\n",
    "        )\n",
    "                \n",
    "    def forward(self, x0):\n",
    "        ## x: (batch, input_size)\n",
    "\n",
    "        batch_size = x0.shape[0]\n",
    "        \n",
    "        ## SHARED NET\n",
    "        \n",
    "        x = self.shared(x0)\n",
    "        \n",
    "        ## CAUSE-SPECIFIC NET\n",
    "        \n",
    "        x1 = self.cs1(torch.cat([x, x0], 1))\n",
    "        x2 = self.cs2(torch.cat([x, x0], 1))\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "        \n",
    "        return torch.softmax(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight_decay in [1e-3]:\n",
    "    for shared_size in [16, 32, 64, 128, 256]:\n",
    "        for cs_size in [16, 32, 64, 128, 256]:\n",
    "\n",
    "            path = './DeepHit_cr_{}statesize_{}cssize_{:.0e}'.format(shared_size, cs_size, weight_decay)\n",
    "            #if os.path.isfile(path):\n",
    "            #    continue\n",
    "\n",
    "            model = deephitCR(input_size=total_train.shape[-1], ntime=ntime, shared_size=shared_size, cs_size=cs_size, num_event=num_event).to(device)\n",
    "            #if os.path.isfile(path):\n",
    "            #    model.load_state_dict(torch.load(path, map_location = device))\n",
    "\n",
    "            lr = 1e-2\n",
    "            optimizer = adabound.AdaBound(model.parameters(), lr=lr, weight_decay=0)\n",
    "\n",
    "            loss_array = []\n",
    "            loss_array1 = []\n",
    "            loss_array2 = []\n",
    "            patience = 0\n",
    "            min_loss = np.inf\n",
    "            for e in range(int(1e6)):\n",
    "\n",
    "                loss1_array_tmp = []\n",
    "                loss2_array_tmp = []\n",
    "\n",
    "                for total_batch, timevar_compact_batch, event_batch, time_batch, obs_mask_batch in train_loader:\n",
    "\n",
    "                    input_lengths_batch = ((timevar_compact_batch[:,:,-1] ).sum(1) + 1).int()\n",
    "\n",
    "                    total_batch = total_batch.float()\n",
    "                    event_batch = event_batch.float()\n",
    "                    time_batch = time_batch.float()\n",
    "                    obs_mask_batch = obs_mask_batch.float()\n",
    "\n",
    "                    mask1_batch = f_get_fc_mask1(input_lengths_batch.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2)\n",
    "                    mask2_batch = f_get_fc_mask2(time_batch.reshape(-1, 1), event_batch.reshape(-1, 1), num_Event=2, num_Category=ntime+2)\n",
    "                    mask3_batch = f_get_fc_mask3(time_batch.reshape(-1, 1), input_lengths_batch.reshape(-1, 1) - 1, ntime+2)\n",
    "\n",
    "                    y_pred = model(total_batch.to(device))\n",
    "\n",
    "                    norm = 0.\n",
    "                    for parameter in model.parameters():\n",
    "                        norm += torch.norm(parameter, p=1)\n",
    "\n",
    "                    loss1, loss2 = custom_loss(pmf = y_pred.reshape(-1, num_event, ntime+2), \n",
    "                                               event = event_batch.reshape(-1, 1).to(device), \n",
    "                                               time = time_batch.reshape(-1, 1).to(device),\n",
    "                                               mask1 = mask1_batch.to(device), \n",
    "                                               mask2 = mask2_batch.to(device), \n",
    "                                               mask3 = mask3_batch.to(device), \n",
    "                                               num_event = num_event)\n",
    "\n",
    "                    loss = loss1 + loss2 + weight_decay*norm\n",
    "                    loss1_array_tmp.append(loss1.item())\n",
    "                    loss2_array_tmp.append(loss2.item())\n",
    "\n",
    "                    model.zero_grad()\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "\n",
    "                loss_array.append(np.mean(loss1_array_tmp) + np.mean(loss2_array_tmp))\n",
    "                loss_array1.append(np.mean(loss1_array_tmp))\n",
    "                loss_array2.append(np.mean(loss2_array_tmp))\n",
    "                if e % 100 == 0:\n",
    "                    print('Epoch: ' + str(e) + \n",
    "                          ', TotalLoss: '+ f'{loss_array[-1]:.4e}' +\n",
    "                          ', Loss1: '+ f'{loss_array1[-1]:.4e}' + \n",
    "                          ', Loss2: '+ f'{loss_array2[-1]:.4e}')\n",
    "                if min_loss > loss_array[-1]:\n",
    "                    patience = 0\n",
    "                    min_loss = loss_array[-1]\n",
    "                    torch.save(model.state_dict(), path)\n",
    "                else:\n",
    "                    patience += 1\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                if patience > 1000:\n",
    "                    break\n",
    "\n",
    "            plt.plot(loss_array, label='Total Loss')\n",
    "            plt.plot(loss_array1, label='Loss 1')\n",
    "            plt.plot(loss_array2, label='Loss 2')\n",
    "            plt.ylabel('loss')\n",
    "            plt.xlabel('epoch')\n",
    "            plt.yscale('log')\n",
    "            plt.title(path[2:])\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "            model.load_state_dict(torch.load(path, map_location = device))\n",
    "            \n",
    "            print(path[9:])\n",
    "            \n",
    "            for delt in [1,3,6,9,12]:\n",
    "\n",
    "                print('****************************************')\n",
    "                print('Delta t = {}'.format(delt))\n",
    "                print('****************************************')\n",
    "\n",
    "                input_lengths_train = torch.IntTensor((timevar_compact_train[:,:,-1] ).sum(1) + 1)\n",
    "\n",
    "                total_train_sort = torch.FloatTensor(total_train)\n",
    "                event_train_sort = torch.FloatTensor(event_train)\n",
    "                time_train_sort = torch.FloatTensor(time_train)\n",
    "                obs_mask_compact_train_sort = torch.FloatTensor(obs_mask_compact_train)\n",
    "\n",
    "                input_lengths_test = torch.IntTensor((timevar_compact_test[:,:,-1] ).sum(1) + 1)\n",
    "\n",
    "                total_test_sort = torch.FloatTensor(total_test)\n",
    "                event_test_sort = torch.FloatTensor(event_test)\n",
    "                time_test_sort = torch.FloatTensor(time_test)\n",
    "                obs_mask_compact_test_sort = torch.FloatTensor(obs_mask_compact_test)\n",
    "\n",
    "                y_train = model(total_train_sort.to(device))\n",
    "                y_test = model(total_test_sort.to(device))\n",
    "\n",
    "                mask1_train_sort = f_get_fc_mask1(input_lengths_train.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2).to(device)\n",
    "                mask1_test_sort = f_get_fc_mask1(input_lengths_test.reshape(-1, 1) - 1, num_Event=2, num_Category=ntime+2).to(device)\n",
    "\n",
    "                CIF_train = torch.cumsum(y_train.reshape(-1,2,ntime+2) * (1-mask1_train_sort), 2)\n",
    "                CIF_test = torch.cumsum(y_test.reshape(-1,2,ntime+2) * (1-mask1_test_sort), 2)\n",
    "\n",
    "                print('test ctd for CVD: {:.4f}'.format(concordance_td(time_test_sort.detach().cpu().numpy(), event_test_sort.detach().cpu().numpy()==1, 1.-CIF_test[:,0,:].detach().cpu().numpy().T, np.int32(time_test_sort.detach().cpu().numpy()-1))))\n",
    "                print('test ctd for Death: {:.4f}'.format(concordance_td(time_test_sort.detach().cpu().numpy(), event_test_sort.detach().cpu().numpy()==2, 1.-CIF_test[:,1,:].detach().cpu().numpy().T, np.int32(time_test_sort.detach().cpu().numpy()-1))) + '\\n')\n",
    "\n",
    "                E_CVD_train = np.array([CIF_train[i,0,int(j)].item() for (i,j) in zip(range(len(time_train_sort)), time_train_sort.detach().cpu().numpy()-1)])\n",
    "                O_CVD_train = event_train_sort.detach().cpu().numpy()==1\n",
    "                E_Death_train = np.array([CIF_train[i,1,int(j)].item() for (i,j) in zip(range(len(time_train_sort)), time_train_sort.detach().cpu().numpy()-1)])\n",
    "                O_Death_train = event_train_sort.detach().cpu().numpy()==2\n",
    "\n",
    "                E_CVD_test = np.array([CIF_test[i,0,int(j)].item() for (i,j) in zip(range(len(time_test_sort)), time_test_sort.detach().cpu().numpy()-1)])\n",
    "                O_CVD_test = event_test_sort.detach().cpu().numpy()==1\n",
    "                E_Death_test = np.array([CIF_test[i,1,int(j)].item() for (i,j) in zip(range(len(time_test_sort)), time_test_sort.detach().cpu().numpy()-1)])\n",
    "                O_Death_test = event_test_sort.detach().cpu().numpy()==2\n",
    "\n",
    "                print('test Brier for CVD: {:.4f}'.format(((E_CVD_test - O_CVD_test)**2).mean()))\n",
    "                print('test Brier for Death: {:.4f}'.format(((E_Death_test - O_Death_test)**2).mean()) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
